llm:
  type: gemini  # ollama, openai, gemini

github:
  token: ${GITHUB_TOKEN}
  api_url: "https://api.github.com"

ollama:
  endpoint: "http://localhost:11434"
  model: "codellama:7b"
  temperature: 0.1
  max_tokens: 2048

openai:
  api_key: ${OPENAI_API_KEY}
  model: "gpt-3.5-turbo"
  temperature: 0.1
  max_tokens: 2048
  api_base: null

gemini:
  api_key: ${GEMINI_API_KEY}
  model: "gemini-2.0-flash"
  temperature: 0.1
  max_tokens: 2048
  api_base: null

review:
  max_files: 50
  max_lines_per_file: 1000
  include_context_lines: 3
  filter_patterns:
    - "*.lock"
    - "*.min.js"
    - "*.generated.*"

output:
  format: "console"  # console, json, markdown
  file: null
  include_summary: true
  show_confidence: true 